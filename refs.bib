@misc{standardization,
  author        = {{Sebastian Raschka}},
  title         = "About Feature Scaling and Normalization
– and the effect of standardization for machine learning algorithms",
  month         = "Jul. 11,", 
  year          = "2014",
  url           = "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
}

@InProceedings{batchnorm,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}

@Article{backprop,
author={Rumelhart, David E.
and Hinton, Geoffrey E.
and Williams, Ronald J.},
title={Learning representations by back-propagating errors},
journal={Nature},
year={1986},
month={Oct},
day={01},
volume={323},
number={6088},
pages={533-536},
abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
issn={1476-4687},
doi={10.1038/323533a0},
url={https://doi.org/10.1038/323533a0}
}

@Article{CLT,
author={Kwak, Sang Gyu
and Kim, Jong Hae},
title={Central limit theorem: the cornerstone of modern statistics},
journal={Korean journal of anesthesiology},
year={2017},
month={Apr},
edition={2017/02/21},
publisher={The Korean Society of Anesthesiologists},
volume={70},
number={2},
pages={144-156},
keywords={Normal distribution; Probability; Statistical distributions; Statistics},
abstract={According to the central limit theorem, the means of a random sample of size, n, from a population with mean, {\textmu}, and variance, $\sigma$(2), distribute normally with mean, {\textmu}, and variance, [Formula: see text]. Using the central limit theorem, a variety of parametric tests have been developed under assumptions about the parameters that determine the population probability distribution. Compared to non-parametric tests, which do not require any assumptions about the population probability distribution, parametric tests produce more accurate and precise estimates with higher statistical powers. However, many medical researchers use parametric tests to present their data without knowledge of the contribution of the central limit theorem to the development of such tests. Thus, this review presents the basic concepts of the central limit theorem and its role in binomial distributions and the Student's t-test, and provides an example of the sampling distributions of small populations. A proof of the central limit theorem is also described with the mathematical concepts required for its near-complete understanding.},
note={28367284[pmid]},
note={PMC5370305[pmcid]},
issn={2005-6419},
doi={10.4097/kjae.2017.70.2.144},
url={https://pubmed.ncbi.nlm.nih.gov/28367284},
url={https://doi.org/10.4097/kjae.2017.70.2.144},
language={eng}
}


﻿@Article{higgs,
author={Baldi, P.
and Sadowski, P.
and Whiteson, D.},
title={Searching for exotic particles in high-energy physics with deep learning},
journal={Nature Communications},
year={2014},
month={Jul},
day={02},
volume={5},
number={1},
pages={4308},
abstract={Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine-learning approaches are often used. Standard approaches have relied on `shallow' machine-learning models that have a limited capacity to learn complex nonlinear functions of the inputs, and rely on a painstaking search through manually constructed nonlinear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Here, using benchmark data sets, we show that deep-learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8{\%} over the best current approaches. This demonstrates that deep-learning approaches can improve the power of collider searches for exotic particles.},
issn={2041-1723},
doi={10.1038/ncomms5308},
url={https://doi.org/10.1038/ncomms5308}
}

@INPROCEEDINGS{bnplacement,
  author={Hasani, Moein and Khotanlou, Hassan},
  booktitle={2019 5th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)}, 
  title={An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ICSPIS48872.2019.9066113}}